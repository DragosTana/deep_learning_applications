from fgsm import fgsm_attack
import torch
import numpy as np  
from model import CNN
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn as nn
from tqdm import tqdm
import matplotlib.pyplot as plt

def evaluate_adversarial_samples(model, loss_fn, dataloader, epsilon, max_iterations=1):
    """
    Evaluate the adversarial samples generated by FGSM attack.
    """
    cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    correct_adv = 0
    l2_norms = []
    linf_norms = []

    #eval on adversarial samples
    for data, label in tqdm(dataloader):
        data, label = data.to(device), label.to(device)

        # random choose a class from cifar10_classes different from label
        target = label.clone()
        while True:
            random_class = torch.randint(0, len(cifar10_classes), (1,)).to(device)
            if random_class != label:
                target = random_class
                break

        original, perturbed = fgsm_attack(model, loss_fn, data, epsilon, ground_truth=label, target=target, max_iterations=max_iterations)
                
        # Evaluate on adversarial samples
        output = model(perturbed)
        pred_adv = output.argmax(dim=1, keepdim=True)
        if pred_adv == label:
            correct_adv += 1

        # Compute perturbation metrics
        perturbation = (perturbed - original).view(len(data), -1)
        l2_norms.extend(torch.norm(perturbation, p=2, dim=1).cpu().numpy())
        linf_norms.extend(torch.norm(perturbation, p=float('inf'), dim=1).cpu().numpy())


    accuracy_adv = correct_adv / len(dataloader.dataset)
    attack_success_rate = 1 - accuracy_adv
    avg_l2_norm = np.mean(l2_norms)
    avg_linf_norm = np.mean(linf_norms)

    return accuracy_adv, attack_success_rate, avg_l2_norm, avg_linf_norm

def main():

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # model
    model = CNN(args=None, num_classes=10)
    model.to(device)
    model.load_state_dict(torch.load('laboratory_4/cifar10_CNN.pth'))

    # data
    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) ])
    batch_size = 1
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=8)

    epsilons = [0.001, 0.005, 0.01, 0.05, 0.1]
    max_iterations = [1]

    accuracies_adv = []
    attack_success_rates = []
    avg_l2_norms = []
    avg_linf_norms = []


    for epsilon in epsilons:
        for max_iter in max_iterations:
            accuracy_adv, attack_success_rate, avg_l2_norm, avg_linf_norm = evaluate_adversarial_samples(model, nn.CrossEntropyLoss(), testloader, epsilon, max_iter)
            accuracies_adv.append(accuracy_adv)
            attack_success_rates.append(attack_success_rate)
            avg_l2_norms.append(avg_l2_norm)
            avg_linf_norms.append(avg_linf_norm)

            print(f'Epsilon: {epsilon}, Max iterations: {max_iter}')
            print(f'Accuracy on adversarial samples: {accuracy_adv}')
            print(f'Attack success rate: {attack_success_rate}')
            print(f'Average L2 norm: {avg_l2_norm}')
            print(f'Average L_inf norm: {avg_linf_norm}')
            print('-----------------------------------')

    #eval on clean samples
    correct_clean = 0   
    for data, label in tqdm(testloader):
        data, label = data.to(device), label.to(device)
        output = model(data)
        pred_clean = output.argmax(dim=1, keepdim=True)
        if pred_clean == label:
            correct_clean += 1
    accuracy_clean = correct_clean / len(testloader.dataset)

    # plots
    fig, axs = plt.subplots(2, 2, figsize=(12, 12))
    fig.suptitle('FGSM attack evaluation')

    # accuracy on adversarial samples as function of epsilon
    axs[0, 0].plot(epsilons, accuracies_adv)
    axs[0, 0].set_xlabel('Epsilon')
    axs[0, 0].set_ylabel('Accuracy')
    axs[0, 0].axhline(y=accuracy_clean, color='r', linestyle='--')
    axs[0, 0].legend(['Accuracy on adversarial samples', 'Accuracy on clean samples'])
    axs[0, 0].set_title('Accuracy on adversarial samples')

    # attack success rate as function of epsilon
    axs[0, 1].plot(epsilons, attack_success_rates)
    axs[0, 1].set_xlabel('Epsilon')
    axs[0, 1].set_ylabel('Attack success rate')
    axs[0, 1].set_title('Attack success rate')

    # average L2 norm as function of epsilon
    axs[1, 0].plot(epsilons, avg_l2_norms)
    axs[1, 0].set_xlabel('Epsilon')
    axs[1, 0].set_ylabel('Average L2 norm')
    axs[1, 0].set_title('Average L2 norm')

    # average L_inf norm as function of epsilon
    axs[1, 1].plot(epsilons, avg_linf_norms)
    axs[1, 1].set_xlabel('Epsilon')
    axs[1, 1].set_ylabel('Average L_inf norm')
    axs[1, 1].set_title('Average L_inf norm')

    plt.show()

    
main()